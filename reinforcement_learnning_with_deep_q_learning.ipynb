{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30775,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datbwoyyy/Egbo-Victor/blob/main/reinforcement_learnning_with_deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-01T05:56:09.242109Z",
          "iopub.execute_input": "2024-10-01T05:56:09.242563Z",
          "iopub.status.idle": "2024-10-01T05:56:10.334655Z",
          "shell.execute_reply.started": "2024-10-01T05:56:09.242520Z",
          "shell.execute_reply": "2024-10-01T05:56:10.333467Z"
        },
        "trusted": true,
        "id": "jSyP8TbfwWyl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN With Pytorch\n",
        "Training a model to to have the optimal policy on control of a system like a cartpole using a reinforcement learning techniques in dynamic environments.\n",
        "* Real-world Applications\n",
        "* Robotics\n",
        "* Game Ai\n",
        "* Autonomous Vehicles\n"
      ],
      "metadata": {
        "id": "dMJyB1ZYwWy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rHha42SxwWy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Define the Neural Network Model for DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self,state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size,24)\n",
        "        self.fc2 = nn.Linear(24,24)\n",
        "        self.fc3 = nn.Linear(24, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99 # Discount factor\n",
        "epsilon = 1.0 # Exploration- Exploitation balance\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "memory_size = 1000000\n",
        "target_update = 10 # Frequency of updating target network\n",
        "\n",
        "# Replay Memory to store experience tuples\n",
        "memory = deque(maxlen= memory_size)\n",
        "\n",
        "# Helper function to choose action using epsilon-greedy policy\n",
        "def choose_action(state, epsilon):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return env.action_space.sample() # Random action exploration\n",
        "    else:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            action_values = model(state)\n",
        "            return np.argmax(action_values.cpu().data.numpy()) # Best action (exploitation)\n",
        "\n",
        "        # Store experience in replay memory\n",
        "        def store_experience(state, action,reward,next_state,done):\n",
        "            memory.append((state,action, reward, next_state,done))\n",
        "\n",
        "        # Train the DQN Model using mini-batch of experiences\n",
        "        def replay():\n",
        "            if len(memory) < batch_size:\n",
        "                return\n",
        "            mini_batch = random.sample(memory, batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
        "\n",
        "            states = torch.FloatTensor(states)\n",
        "            actions = torch.LongTensor(actions)\n",
        "            rewards = torch,FloatTensor(rewards)\n",
        "            next_states = torch.FloatTensor(next_states)\n",
        "            dones = torch.FloatTensor(dones)\n",
        "\n",
        "            # Q- valuses for current states\n",
        "            current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Q-values for next states from target network\n",
        "            with torch.no_grad():\n",
        "                next_q_values = target_model(next_states).max(1)[0]\n",
        "\n",
        "                # Compute target Q-values\n",
        "                target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "                # Loss Function\n",
        "                loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "\n",
        "                # Perform gradient descent\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Initialize models and optimizer\n",
        "            state_size = env.observation_space.shape[0]\n",
        "            action_size = env.action_space.n\n",
        "            model = DQN(state_size, action_size)\n",
        "            target_model = DQN(state_size, action_size)\n",
        "            target_model = DQN(state_size, action_size)\n",
        "            target_model.load_state_dict(model.state_dict()) # Synchronize target model with main model\n",
        "            optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "            # Main training loop\n",
        "            num_episodes = 500\n",
        "            for episode in range(num_episodes):\n",
        "                state = env.reset()\n",
        "                total_reward += reward\n",
        "\n",
        "                store_experience(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "\n",
        "                replay() # Trains the model\n",
        "\n",
        "                if done:\n",
        "                    print(f\"Episode: {episode+1}, Total Reward: {total_reward}, Epsilon:{epsilon:.4f}\")\n",
        "\n",
        "                    # Reduce epsilon for less exploration over tine\n",
        "                    if epsilon > epsilon_min:\n",
        "                        epsilon *= epsilon_decay\n",
        "\n",
        "                    # Update target network every few episodes\n",
        "                    if episode %target_update ==0:\n",
        "                        target_model.load_state_dict(model.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6RBLGrwwWzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42c3c18-f11f-4237-a68c-eb85387b3e2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZCQc06H0xDL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Agent's Performance"
      ],
      "metadata": {
        "id": "lvsjFVbo2vtU"
      }
    },
    {
      "source": [
        "# Assuming your state has 2 features and you have 24 neurons in the first hidden layer\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 24) # Changed input size to match state_size\n",
        "        self.fc2 = nn.Linear(24, 24)\n",
        "        self.fc3 = nn.Linear(24, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cO4Oa03H4Sx",
        "outputId": "1d789d65-2769-4f86-df64-5b9250b90ad6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Define the Neural Network Model for DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self,state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size,24)\n",
        "        self.fc2 = nn.Linear(24,24)\n",
        "        self.fc3 = nn.Linear(24, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99 # Discount factor\n",
        "epsilon = 1.0 # Exploration- Exploitation balance\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "memory_size = 1000000\n",
        "target_update = 10 # Frequency of updating target network\n",
        "\n",
        "# Replay Memory to store experience tuples\n",
        "memory = deque(maxlen= memory_size)\n",
        "\n",
        "# Helper function to choose action using epsilon-greedy policy\n",
        "def choose_action(state, epsilon):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return env.action_space.sample() # Random action exploration\n",
        "    else:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            action_values = model(state)\n",
        "            return np.argmax(action_values.cpu().data.numpy()) # Best action (exploitation)\n",
        "\n",
        "# Store experience in replay memory\n",
        "def store_experience(state, action,reward,next_state,done):\n",
        "    memory.append((state,action, reward, next_state,done))\n",
        "\n",
        "# Train the DQN Model using mini-batch of experiences\n",
        "def replay():\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    mini_batch = random.sample(memory, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
        "\n",
        "    states = torch.FloatTensor(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    rewards = torch.FloatTensor(rewards) # Corrected the typo here from torch,FloatTensor to torch.FloatTensor\n",
        "    next_states = torch.FloatTensor(next_states)\n",
        "    dones = torch.FloatTensor(dones)\n",
        "\n",
        "    # Q- valuses for current states\n",
        "    current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Q-values for next states from target network\n",
        "    with torch.no_grad():\n",
        "        next_q_values = target_model(next_states).max(1)[0]\n",
        "\n",
        "        # Compute target Q-values\n",
        "        target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "        # Loss Function\n",
        "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "\n",
        "        # Perform gradient descent\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Initialize models and optimizer\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "model = DQN(state_size, action_size)\n",
        "target_model = DQN(state_size, action_size)\n",
        "target_model = DQN(state_size, action_size)\n",
        "target_model.load_state_dict(model.state_dict()) # Synchronize target model with main model\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Main training loop\n",
        "num_episodes = 5"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WYmX22qAHbKQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('MountainCar-v0')"
      ],
      "metadata": {
        "id": "2VHh2BjH3liL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1358f3-71b3-4068-ce38-355ea4614fa0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'dqn_cartpole.pth')"
      ],
      "metadata": {
        "id": "CwtgYGbAA3oL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('dqn_cartpole.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvNSKFApA4ez",
        "outputId": "2e3e5fd7-fe16-484d-c97c-58a75751d863"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-8cedfcc93ce3>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('dqn_cartpole.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Visualizations:"
      ],
      "metadata": {
        "id": "zWzAqgsDBBBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import imageio\n",
        "\n",
        "def save_gif(agent, filename='dqn_cartpole.gif'):\n",
        "    frames = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        action = agent(state)\n",
        "        state, _, done, _ = env.step(action)\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(cartpole, frames, fps=30)"
      ],
      "metadata": {
        "id": "QGFOChlYBAxh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import imageio\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "frames = []\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    frames.append(env.render(mode=\"rgb_array\"))  # Collect frames\n",
        "    action = choose_action(state, epsilon=0.0)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Save as a GIF\n",
        "imageio.mimsave('dqn_cartpole.gif', frames, fps=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrdcPAOTrjc_",
        "outputId": "435860df-416b-4101-8dd5-6b68cde1572f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/imageio/plugins/pillow.py:409: DeprecationWarning: The keyword `fps` is no longer supported. Use `duration`(in ms) instead, e.g. `fps=50` == `duration=20` (1000 * 1/50).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('MountainCar-v0')"
      ],
      "metadata": {
        "id": "KqZaE3NzqRwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5929b0b9-702c-458b-f9d4-c1ccfa2ba268"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'MountainCar-v0')"
      ],
      "metadata": {
        "id": "5_VL4Rg-sLPr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4hVzdI6s4dT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}